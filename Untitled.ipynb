{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ed4abe-8b73-444e-a074-91fa85e961bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "import joblib\n",
    "from sklearn import datasets\n",
    "from skimage.feature import hog\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c21c9e-aa02-480e-b68a-36f9320d10ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "# Extract the features and labels\n",
    "features = train_images # Use train_images as features\n",
    "labels = train_labels # Use train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5435119c-fcb3-4778-a03d-51b2b0bd61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hog features\n",
    "list_hog_ft = []\n",
    "for feature in features:\n",
    " ft = hog(feature.reshape((28, 28)), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualize=False)\n",
    " list_hog_ft.append(ft)\n",
    "hog_features = np.array(list_hog_ft, 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05e73ea0-e53f-46df-8abc-441c49437d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "pp = preprocessing.StandardScaler().fit(hog_features)\n",
    "hog_features = pp.transform(hog_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c8e361-b829-4af8-b8c1-14939905190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47334096\n",
      "Iteration 2, loss = 0.33204082\n",
      "Iteration 3, loss = 0.30693697\n",
      "Iteration 4, loss = 0.29180112\n",
      "Iteration 5, loss = 0.28243525\n",
      "Iteration 6, loss = 0.27477578\n",
      "Iteration 7, loss = 0.26806963\n",
      "Iteration 8, loss = 0.26348726\n",
      "Iteration 9, loss = 0.25932434\n",
      "Iteration 10, loss = 0.25481097\n",
      "Iteration 11, loss = 0.25090791\n",
      "Iteration 12, loss = 0.24787521\n",
      "Iteration 13, loss = 0.24489655\n",
      "Iteration 14, loss = 0.24214817\n",
      "Iteration 15, loss = 0.24091562\n",
      "Iteration 16, loss = 0.23818261\n",
      "Iteration 17, loss = 0.23710987\n",
      "Iteration 18, loss = 0.23570235\n",
      "Iteration 19, loss = 0.23219617\n",
      "Iteration 20, loss = 0.23428633\n",
      "Iteration 21, loss = 0.22995106\n",
      "Iteration 22, loss = 0.22867770\n",
      "Iteration 23, loss = 0.22931534\n",
      "Iteration 24, loss = 0.22772033\n",
      "Iteration 25, loss = 0.22668544\n",
      "Iteration 26, loss = 0.22349429\n",
      "Iteration 27, loss = 0.22325047\n",
      "Iteration 28, loss = 0.22211116\n",
      "Iteration 29, loss = 0.22237731\n",
      "Iteration 30, loss = 0.21986002\n",
      "Iteration 31, loss = 0.21838774\n",
      "Iteration 32, loss = 0.21881204\n",
      "Iteration 33, loss = 0.22027847\n",
      "Iteration 34, loss = 0.21801355\n",
      "Iteration 35, loss = 0.21596158\n",
      "Iteration 36, loss = 0.21603601\n",
      "Iteration 37, loss = 0.21590914\n",
      "Iteration 38, loss = 0.21599339\n",
      "Iteration 39, loss = 0.21343150\n",
      "Iteration 40, loss = 0.21256522\n",
      "Iteration 41, loss = 0.21275480\n",
      "Iteration 42, loss = 0.21169897\n",
      "Iteration 43, loss = 0.21332842\n",
      "Iteration 44, loss = 0.21126230\n",
      "Iteration 45, loss = 0.21152610\n",
      "Iteration 46, loss = 0.20940734\n",
      "Iteration 47, loss = 0.20894182\n",
      "Iteration 48, loss = 0.21093306\n",
      "Iteration 49, loss = 0.20749466\n",
      "Iteration 50, loss = 0.20867849\n",
      "Iteration 51, loss = 0.20786258\n",
      "Iteration 52, loss = 0.20775059\n",
      "Iteration 53, loss = 0.20581242\n",
      "Iteration 54, loss = 0.20659168\n",
      "Iteration 55, loss = 0.20689334\n",
      "Iteration 56, loss = 0.20609057\n",
      "Iteration 57, loss = 0.20624286\n",
      "Iteration 58, loss = 0.20588484\n",
      "Iteration 59, loss = 0.20561199\n",
      "Iteration 60, loss = 0.20540665\n",
      "Iteration 61, loss = 0.20640350\n",
      "Iteration 62, loss = 0.20452578\n",
      "Iteration 63, loss = 0.20339425\n",
      "Iteration 64, loss = 0.20391566\n",
      "Iteration 65, loss = 0.20312255\n",
      "Iteration 66, loss = 0.20413755\n",
      "Iteration 67, loss = 0.20437809\n",
      "Iteration 68, loss = 0.20456640\n",
      "Iteration 69, loss = 0.20406562\n",
      "Iteration 70, loss = 0.20396183\n",
      "Iteration 71, loss = 0.20383213\n",
      "Iteration 72, loss = 0.20299759\n",
      "Iteration 73, loss = 0.20280801\n",
      "Iteration 74, loss = 0.20238569\n",
      "Iteration 75, loss = 0.20085321\n",
      "Iteration 76, loss = 0.20294864\n",
      "Iteration 77, loss = 0.20138250\n",
      "Iteration 78, loss = 0.20091655\n",
      "Iteration 79, loss = 0.20003437\n",
      "Iteration 80, loss = 0.20266005\n",
      "Iteration 81, loss = 0.19971048\n",
      "Iteration 82, loss = 0.20049318\n",
      "Iteration 83, loss = 0.20199911\n",
      "Iteration 84, loss = 0.20076108\n",
      "Iteration 85, loss = 0.20065953\n",
      "Iteration 86, loss = 0.19873953\n",
      "Iteration 87, loss = 0.20054259\n",
      "Iteration 88, loss = 0.19816826\n",
      "Iteration 89, loss = 0.19837688\n",
      "Iteration 90, loss = 0.19840405\n",
      "Iteration 91, loss = 0.19675715\n",
      "Iteration 92, loss = 0.19855505\n",
      "Iteration 93, loss = 0.19889483\n",
      "Iteration 94, loss = 0.19720932\n",
      "Iteration 95, loss = 0.19898518\n",
      "Iteration 96, loss = 0.19812344\n",
      "Iteration 97, loss = 0.19742526\n",
      "Iteration 98, loss = 0.19594213\n",
      "Iteration 99, loss = 0.19865880\n",
      "Iteration 100, loss = 0.19823141\n",
      "Iteration 101, loss = 0.19594831\n",
      "Iteration 102, loss = 0.19750840\n",
      "Iteration 103, loss = 0.19663339\n",
      "Iteration 104, loss = 0.19571509\n",
      "Iteration 105, loss = 0.19507734\n",
      "Iteration 106, loss = 0.19533244\n",
      "Iteration 107, loss = 0.19730286\n",
      "Iteration 108, loss = 0.19419957\n",
      "Iteration 109, loss = 0.19470242\n",
      "Iteration 110, loss = 0.19601118\n",
      "Iteration 111, loss = 0.19623784\n",
      "Iteration 112, loss = 0.19579848\n",
      "Iteration 113, loss = 0.19703312\n",
      "Iteration 114, loss = 0.19332318\n",
      "Iteration 115, loss = 0.19457539\n",
      "Iteration 116, loss = 0.19541773\n",
      "Iteration 117, loss = 0.19376151\n",
      "Iteration 118, loss = 0.19480697\n",
      "Iteration 119, loss = 0.19608116\n",
      "Iteration 120, loss = 0.19457178\n",
      "Iteration 121, loss = 0.19247292\n",
      "Iteration 122, loss = 0.19440859\n",
      "Iteration 123, loss = 0.19363939\n",
      "Iteration 124, loss = 0.19564104\n",
      "Iteration 125, loss = 0.19199800\n",
      "Iteration 126, loss = 0.19452768\n",
      "Iteration 127, loss = 0.19313583\n",
      "Iteration 128, loss = 0.19300829\n",
      "Iteration 129, loss = 0.19132542\n",
      "Iteration 130, loss = 0.19284424\n",
      "Iteration 131, loss = 0.19350672\n",
      "Iteration 132, loss = 0.19377628\n",
      "Iteration 133, loss = 0.18962159\n",
      "Iteration 134, loss = 0.19379941\n",
      "Iteration 135, loss = 0.19270250\n",
      "Iteration 136, loss = 0.19254699\n",
      "Iteration 137, loss = 0.19261584\n",
      "Iteration 138, loss = 0.19136442\n",
      "Iteration 139, loss = 0.19212725\n",
      "Iteration 140, loss = 0.19110908\n",
      "Iteration 141, loss = 0.19292692\n",
      "Iteration 142, loss = 0.19191583\n",
      "Iteration 143, loss = 0.19283127\n",
      "Iteration 144, loss = 0.19265187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 89.43%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(hog_features, labels, test_size=0.2, random_state=42)\n",
    "# Create an MLP classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, alpha=1e-4,\n",
    " solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    " learning_rate_init=0.1)\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a8f622f-b307-4b46-aa21-eed589eef741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93165"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(hog_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ca2226-bae2-4460-86e3-fd33380bec0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['digits_cls1.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the classifier\n",
    "joblib.dump((clf, pp), \"digits_cls1.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62b7f293-a94e-4d5d-b847-fe7771136ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47334096\n",
      "Iteration 2, loss = 0.33204082\n",
      "Iteration 3, loss = 0.30693697\n",
      "Iteration 4, loss = 0.29180112\n",
      "Iteration 5, loss = 0.28243525\n",
      "Iteration 6, loss = 0.27477578\n",
      "Iteration 7, loss = 0.26806963\n",
      "Iteration 8, loss = 0.26348726\n",
      "Iteration 9, loss = 0.25932434\n",
      "Iteration 10, loss = 0.25481097\n",
      "Iteration 11, loss = 0.25090791\n",
      "Iteration 12, loss = 0.24787521\n",
      "Iteration 13, loss = 0.24489655\n",
      "Iteration 14, loss = 0.24214817\n",
      "Iteration 15, loss = 0.24091562\n",
      "Iteration 16, loss = 0.23818261\n",
      "Iteration 17, loss = 0.23710987\n",
      "Iteration 18, loss = 0.23570235\n",
      "Iteration 19, loss = 0.23219617\n",
      "Iteration 20, loss = 0.23428633\n",
      "Iteration 21, loss = 0.22995106\n",
      "Iteration 22, loss = 0.22867770\n",
      "Iteration 23, loss = 0.22931534\n",
      "Iteration 24, loss = 0.22772033\n",
      "Iteration 25, loss = 0.22668544\n",
      "Iteration 26, loss = 0.22349429\n",
      "Iteration 27, loss = 0.22325047\n",
      "Iteration 28, loss = 0.22211116\n",
      "Iteration 29, loss = 0.22237731\n",
      "Iteration 30, loss = 0.21986002\n",
      "Iteration 31, loss = 0.21838774\n",
      "Iteration 32, loss = 0.21881204\n",
      "Iteration 33, loss = 0.22027847\n",
      "Iteration 34, loss = 0.21801355\n",
      "Iteration 35, loss = 0.21596158\n",
      "Iteration 36, loss = 0.21603601\n",
      "Iteration 37, loss = 0.21590914\n",
      "Iteration 38, loss = 0.21599339\n",
      "Iteration 39, loss = 0.21343150\n",
      "Iteration 40, loss = 0.21256522\n",
      "Iteration 41, loss = 0.21275480\n",
      "Iteration 42, loss = 0.21169897\n",
      "Iteration 43, loss = 0.21332842\n",
      "Iteration 44, loss = 0.21126230\n",
      "Iteration 45, loss = 0.21152610\n",
      "Iteration 46, loss = 0.20940734\n",
      "Iteration 47, loss = 0.20894182\n",
      "Iteration 48, loss = 0.21093306\n",
      "Iteration 49, loss = 0.20749466\n",
      "Iteration 50, loss = 0.20867849\n",
      "Iteration 51, loss = 0.20786258\n",
      "Iteration 52, loss = 0.20775059\n",
      "Iteration 53, loss = 0.20581242\n",
      "Iteration 54, loss = 0.20659168\n",
      "Iteration 55, loss = 0.20689334\n",
      "Iteration 56, loss = 0.20609057\n",
      "Iteration 57, loss = 0.20624286\n",
      "Iteration 58, loss = 0.20588484\n",
      "Iteration 59, loss = 0.20561199\n",
      "Iteration 60, loss = 0.20540665\n",
      "Iteration 61, loss = 0.20640350\n",
      "Iteration 62, loss = 0.20452578\n",
      "Iteration 63, loss = 0.20339425\n",
      "Iteration 64, loss = 0.20391566\n",
      "Iteration 65, loss = 0.20312255\n",
      "Iteration 66, loss = 0.20413755\n",
      "Iteration 67, loss = 0.20437809\n",
      "Iteration 68, loss = 0.20456640\n",
      "Iteration 69, loss = 0.20406562\n",
      "Iteration 70, loss = 0.20396183\n",
      "Iteration 71, loss = 0.20383213\n",
      "Iteration 72, loss = 0.20299759\n",
      "Iteration 73, loss = 0.20280801\n",
      "Iteration 74, loss = 0.20238569\n",
      "Iteration 75, loss = 0.20085321\n",
      "Iteration 76, loss = 0.20294864\n",
      "Iteration 77, loss = 0.20138250\n",
      "Iteration 78, loss = 0.20091655\n",
      "Iteration 79, loss = 0.20003437\n",
      "Iteration 80, loss = 0.20266005\n",
      "Iteration 81, loss = 0.19971048\n",
      "Iteration 82, loss = 0.20049318\n",
      "Iteration 83, loss = 0.20199911\n",
      "Iteration 84, loss = 0.20076108\n",
      "Iteration 85, loss = 0.20065953\n",
      "Iteration 86, loss = 0.19873953\n",
      "Iteration 87, loss = 0.20054259\n",
      "Iteration 88, loss = 0.19816826\n",
      "Iteration 89, loss = 0.19837688\n",
      "Iteration 90, loss = 0.19840405\n",
      "Iteration 91, loss = 0.19675715\n",
      "Iteration 92, loss = 0.19855505\n",
      "Iteration 93, loss = 0.19889483\n",
      "Iteration 94, loss = 0.19720932\n",
      "Iteration 95, loss = 0.19898518\n",
      "Iteration 96, loss = 0.19812344\n",
      "Iteration 97, loss = 0.19742526\n",
      "Iteration 98, loss = 0.19594213\n",
      "Iteration 99, loss = 0.19865880\n",
      "Iteration 100, loss = 0.19823141\n",
      "Iteration 101, loss = 0.19594831\n",
      "Iteration 102, loss = 0.19750840\n",
      "Iteration 103, loss = 0.19663339\n",
      "Iteration 104, loss = 0.19571509\n",
      "Iteration 105, loss = 0.19507734\n",
      "Iteration 106, loss = 0.19533244\n",
      "Iteration 107, loss = 0.19730286\n",
      "Iteration 108, loss = 0.19419957\n",
      "Iteration 109, loss = 0.19470242\n",
      "Iteration 110, loss = 0.19601118\n",
      "Iteration 111, loss = 0.19623784\n",
      "Iteration 112, loss = 0.19579848\n",
      "Iteration 113, loss = 0.19703312\n",
      "Iteration 114, loss = 0.19332318\n",
      "Iteration 115, loss = 0.19457539\n",
      "Iteration 116, loss = 0.19541773\n",
      "Iteration 117, loss = 0.19376151\n",
      "Iteration 118, loss = 0.19480697\n",
      "Iteration 119, loss = 0.19608116\n",
      "Iteration 120, loss = 0.19457178\n",
      "Iteration 121, loss = 0.19247292\n",
      "Iteration 122, loss = 0.19440859\n",
      "Iteration 123, loss = 0.19363939\n",
      "Iteration 124, loss = 0.19564104\n",
      "Iteration 125, loss = 0.19199800\n",
      "Iteration 126, loss = 0.19452768\n",
      "Iteration 127, loss = 0.19313583\n",
      "Iteration 128, loss = 0.19300829\n",
      "Iteration 129, loss = 0.19132542\n",
      "Iteration 130, loss = 0.19284424\n",
      "Iteration 131, loss = 0.19350672\n",
      "Iteration 132, loss = 0.19377628\n",
      "Iteration 133, loss = 0.18962159\n",
      "Iteration 134, loss = 0.19379941\n",
      "Iteration 135, loss = 0.19270250\n",
      "Iteration 136, loss = 0.19254699\n",
      "Iteration 137, loss = 0.19261584\n",
      "Iteration 138, loss = 0.19136442\n",
      "Iteration 139, loss = 0.19212725\n",
      "Iteration 140, loss = 0.19110908\n",
      "Iteration 141, loss = 0.19292692\n",
      "Iteration 142, loss = 0.19191583\n",
      "Iteration 143, loss = 0.19283127\n",
      "Iteration 144, loss = 0.19265187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 89.43%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['digits_cls1_pp.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from skimage.feature import hog\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Extract the features and labels\n",
    "features = train_images\n",
    "labels = train_labels\n",
    "\n",
    "# Extract the HOG features\n",
    "list_hog_ft = []\n",
    "for feature in features:\n",
    "    ft = hog(feature.reshape((28, 28)), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualize=False)\n",
    "    list_hog_ft.append(ft)\n",
    "hog_features = np.array(list_hog_ft, 'float64')\n",
    "\n",
    "# Normalize the features\n",
    "pp = preprocessing.StandardScaler().fit(hog_features)\n",
    "hog_features = pp.transform(hog_features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(hog_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an MLP classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, alpha=1e-4, solver='sgd', verbose=10, tol=1e-4, random_state=1, learning_rate_init=0.1)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the classifier and preprocessor\n",
    "joblib.dump((clf, pp), \"digits_cls1.pkl\", compress=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6382aca-0d80-4bb9-a59d-af032d12ce92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUSS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6939 - loss: 1.1472 - val_accuracy: 0.8986 - val_loss: 0.3823\n",
      "Epoch 2/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8981 - loss: 0.3795 - val_accuracy: 0.9138 - val_loss: 0.3103\n",
      "Epoch 3/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9118 - loss: 0.3214 - val_accuracy: 0.9223 - val_loss: 0.2784\n",
      "Epoch 4/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9199 - loss: 0.2859 - val_accuracy: 0.9285 - val_loss: 0.2602\n",
      "Epoch 5/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9247 - loss: 0.2681 - val_accuracy: 0.9333 - val_loss: 0.2394\n",
      "Epoch 6/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9316 - loss: 0.2443 - val_accuracy: 0.9373 - val_loss: 0.2250\n",
      "Epoch 7/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9348 - loss: 0.2302 - val_accuracy: 0.9420 - val_loss: 0.2129\n",
      "Epoch 8/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9401 - loss: 0.2091 - val_accuracy: 0.9450 - val_loss: 0.2027\n",
      "Epoch 9/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9454 - loss: 0.1965 - val_accuracy: 0.9470 - val_loss: 0.1923\n",
      "Epoch 10/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9470 - loss: 0.1876 - val_accuracy: 0.9491 - val_loss: 0.1856\n",
      "Epoch 11/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9496 - loss: 0.1785 - val_accuracy: 0.9515 - val_loss: 0.1764\n",
      "Epoch 12/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9536 - loss: 0.1717 - val_accuracy: 0.9518 - val_loss: 0.1708\n",
      "Epoch 13/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9550 - loss: 0.1619 - val_accuracy: 0.9538 - val_loss: 0.1652\n",
      "Epoch 14/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9563 - loss: 0.1580 - val_accuracy: 0.9565 - val_loss: 0.1589\n",
      "Epoch 15/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9588 - loss: 0.1465 - val_accuracy: 0.9565 - val_loss: 0.1553\n",
      "Epoch 16/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9615 - loss: 0.1372 - val_accuracy: 0.9594 - val_loss: 0.1490\n",
      "Epoch 17/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9632 - loss: 0.1372 - val_accuracy: 0.9595 - val_loss: 0.1458\n",
      "Epoch 18/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1317 - val_accuracy: 0.9609 - val_loss: 0.1422\n",
      "Epoch 19/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9660 - loss: 0.1245 - val_accuracy: 0.9602 - val_loss: 0.1398\n",
      "Epoch 20/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9661 - loss: 0.1211 - val_accuracy: 0.9619 - val_loss: 0.1357\n",
      "Epoch 21/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9663 - loss: 0.1235 - val_accuracy: 0.9622 - val_loss: 0.1334\n",
      "Epoch 22/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9699 - loss: 0.1122 - val_accuracy: 0.9638 - val_loss: 0.1308\n",
      "Epoch 23/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9692 - loss: 0.1150 - val_accuracy: 0.9635 - val_loss: 0.1285\n",
      "Epoch 24/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.1060 - val_accuracy: 0.9641 - val_loss: 0.1261\n",
      "Epoch 25/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.1047 - val_accuracy: 0.9643 - val_loss: 0.1247\n",
      "Epoch 26/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9732 - loss: 0.1007 - val_accuracy: 0.9651 - val_loss: 0.1213\n",
      "Epoch 27/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0971 - val_accuracy: 0.9657 - val_loss: 0.1199\n",
      "Epoch 28/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9735 - loss: 0.0979 - val_accuracy: 0.9661 - val_loss: 0.1180\n",
      "Epoch 29/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.0912 - val_accuracy: 0.9664 - val_loss: 0.1172\n",
      "Epoch 30/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9759 - loss: 0.0905 - val_accuracy: 0.9665 - val_loss: 0.1152\n",
      "Epoch 31/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0859 - val_accuracy: 0.9671 - val_loss: 0.1151\n",
      "Epoch 32/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.0857 - val_accuracy: 0.9668 - val_loss: 0.1132\n",
      "Epoch 33/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.0830 - val_accuracy: 0.9682 - val_loss: 0.1116\n",
      "Epoch 34/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9796 - loss: 0.0776 - val_accuracy: 0.9681 - val_loss: 0.1102\n",
      "Epoch 35/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.0812 - val_accuracy: 0.9681 - val_loss: 0.1099\n",
      "Epoch 36/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.0781 - val_accuracy: 0.9685 - val_loss: 0.1079\n",
      "Epoch 37/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9792 - loss: 0.0780 - val_accuracy: 0.9688 - val_loss: 0.1067\n",
      "Epoch 38/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9819 - loss: 0.0712 - val_accuracy: 0.9689 - val_loss: 0.1064\n",
      "Epoch 39/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0754 - val_accuracy: 0.9693 - val_loss: 0.1047\n",
      "Epoch 40/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0722 - val_accuracy: 0.9701 - val_loss: 0.1034\n",
      "Epoch 41/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0691 - val_accuracy: 0.9702 - val_loss: 0.1027\n",
      "Epoch 42/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9831 - loss: 0.0651 - val_accuracy: 0.9698 - val_loss: 0.1025\n",
      "Epoch 43/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0659 - val_accuracy: 0.9706 - val_loss: 0.1012\n",
      "Epoch 44/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0661 - val_accuracy: 0.9708 - val_loss: 0.1007\n",
      "Epoch 45/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0652 - val_accuracy: 0.9707 - val_loss: 0.0994\n",
      "Epoch 46/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9854 - loss: 0.0604 - val_accuracy: 0.9703 - val_loss: 0.0992\n",
      "Epoch 47/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9839 - loss: 0.0628 - val_accuracy: 0.9710 - val_loss: 0.0991\n",
      "Epoch 48/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9839 - loss: 0.0590 - val_accuracy: 0.9711 - val_loss: 0.0970\n",
      "Epoch 49/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0573 - val_accuracy: 0.9707 - val_loss: 0.0983\n",
      "Epoch 50/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0566 - val_accuracy: 0.9725 - val_loss: 0.0956\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9698 - loss: 0.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.39%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Extract the features and labels\n",
    "X_train = train_images.reshape(-1, 28, 28)  # Reshape to fit HOG feature extraction\n",
    "X_test = test_images.reshape(-1, 28, 28)\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "\n",
    "# Extract the HOG features (not using HOG, but just as a placeholder for feature extraction)\n",
    "# Here, we use the raw pixel values as features\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Build the MLP model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc * 100:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "model.save('mnist_mlp_model.h5')\n",
    "\n",
    "# Save the preprocessor (scaling is not needed in this case)\n",
    "# Here, we don't need to save any preprocessor since we are using raw pixel values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f35b71-d794-4f3d-9ad7-e169e4798e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
